{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------+\n",
      "|TerritoryID|TerritoryDescription|RegionID|\n",
      "+-----------+--------------------+--------+\n",
      "|       1581|            Westboro|       1|\n",
      "|       1730|             Bedford|       1|\n",
      "|       1833|           Georgetow|       1|\n",
      "|       2116|              Boston|       1|\n",
      "|       2139|           Cambridge|       1|\n",
      "+-----------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark # Call this only after findspark.init()\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# NOTE: header(default = False) uses column name as c0,c1....but with header = True it will give us first line as our header.\n",
    "\n",
    "\n",
    "df = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\territories.csv\",inferSchema = True, header = True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the datatypes used in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BinaryType: binary\n",
      "BooleanType: boolean\n",
      "ByteType: tinyint\n",
      "DateType: date\n",
      "DecimalType: decimal(10,0)\n",
      "DoubleType: double\n",
      "FloatType: float\n",
      "IntegerType: int\n",
      "LongType: bigint\n",
      "ShortType: smallint\n",
      "StringType: string\n",
      "TimestampType: timestamp\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types \n",
    "\n",
    "for t in ['BinaryType', 'BooleanType', 'ByteType', 'DateType', \n",
    "          'DecimalType', 'DoubleType', 'FloatType', 'IntegerType', \n",
    "           'LongType', 'ShortType', 'StringType', 'TimestampType']:\n",
    "    print(f\"{t}: {getattr(types, t)().simpleString()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will change the datatype as given in ER diagrm by using cast function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is dataframe for our territories tabel\n",
    "\n",
    "df = df.withColumn(\"RegionID\",df[\"RegionID\"].cast(IntegerType()))\n",
    "df = df.withColumn(\"TerritoryID\",df[\"TerritoryID\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printSchema() function shows the datatype of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- TerritoryID: string (nullable = true)\n",
      " |-- TerritoryDescription: string (nullable = true)\n",
      " |-- RegionID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|RegionID|   RegionDescription|\n",
      "+--------+--------------------+\n",
      "|       1|Eastern          ...|\n",
      "|       2|Western          ...|\n",
      "|       3|Northern         ...|\n",
      "|       4|            Southern|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df2 is dataframe for our regions tabel\n",
    "\n",
    "df2 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\regions.csv\",inferSchema = True, header = True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumn(\"RegionID\",df2[\"RegionID\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- RegionID: integer (nullable = true)\n",
      " |-- RegionDescription: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|EmployeeID|TerritoryID|\n",
      "+----------+-----------+\n",
      "|         1|       6897|\n",
      "|         1|      19713|\n",
      "|         2|       1581|\n",
      "|         2|       1730|\n",
      "|         2|       1833|\n",
      "|         2|       2116|\n",
      "|         2|       2139|\n",
      "|         2|       2184|\n",
      "|         2|      40222|\n",
      "|         3|      30346|\n",
      "|         3|      31406|\n",
      "|         3|      32859|\n",
      "|         3|      33607|\n",
      "|         4|      20852|\n",
      "|         4|      27403|\n",
      "|         4|      27511|\n",
      "|         5|       2903|\n",
      "|         5|       7960|\n",
      "|         5|       8837|\n",
      "|         5|      10019|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df3 is dataframe for our employee-territories tabel\n",
    "\n",
    "df3 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\employee-territories.csv\",inferSchema = True, header = True)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- TerritoryID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df3.withColumn(\"EmployeeID\",df3[\"EmployeeID\"].cast(IntegerType()))\n",
    "df3 = df3.withColumn(\"TerritoryID\",df3[\"TerritoryID\"].cast(StringType()))\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+--------------------+---------------+-------------------+-------------------+--------------------+--------+------+----------+-------+--------------+---------+--------------------+--------------------+---------+--------------------+\n",
      "|EmployeeID| LastName|FirstName|               Title|TitleOfCourtesy|          BirthDate|           HireDate|             Address|    City|Region|PostalCode|Country|     HomePhone|Extension|               Photo|               Notes|ReportsTo|           PhotoPath|\n",
      "+----------+---------+---------+--------------------+---------------+-------------------+-------------------+--------------------+--------+------+----------+-------+--------------+---------+--------------------+--------------------+---------+--------------------+\n",
      "|         1|  Davolio|    Nancy|Sales Representative|            Ms.|1948-12-08 00:00:00|1992-05-01 00:00:00|507 - 20th Ave. E...| Seattle|    WA|     98122|    USA|(206) 555-9857|     5467|0x151C2F000200000...|Education include...|        2|http://accweb/emm...|\n",
      "|         2|   Fuller|   Andrew|Vice President Sales|            Dr.|1952-02-19 00:00:00|1992-08-14 00:00:00|  908 W. Capital Way|  Tacoma|    WA|     98401|    USA|(206) 555-9482|     3457|0x151C2F000200000...|Andrew received h...|     NULL|http://accweb/emm...|\n",
      "|         3|Leverling|    Janet|Sales Representative|            Ms.|1963-08-30 00:00:00|1992-04-01 00:00:00|  722 Moss Bay Blvd.|Kirkland|    WA|     98033|    USA|(206) 555-3412|     3355|0x151C2F000200000...|Janet has a BS de...|        2|http://accweb/emm...|\n",
      "+----------+---------+---------+--------------------+---------------+-------------------+-------------------+--------------------+--------+------+----------+-------+--------------+---------+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df4 is dataframe for our employee tabel\n",
    "\n",
    "df4 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\employees.csv\",inferSchema = True, header = True)\n",
    "df4.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.withColumn(\"EmployeeID\",df4[\"EmployeeID\"].cast(IntegerType()))\n",
    "df4 = df4.withColumn(\"LastName\",df4[\"LastName\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"FirstName\",df4[\"FirstName\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"Title\",df4[\"Title\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"TitleOfCourtesy\",df4[\"TitleOfCourtesy\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"BirthDate\",df4[\"BirthDate\"].cast(DateType()))\n",
    "df4 = df4.withColumn(\"HireDate\",df4[\"HireDate\"].cast(DateType()))\n",
    "df4 = df4.withColumn(\"Address\",df4[\"Address\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"City\",df4[\"City\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"Region\",df4[\"Region\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"PostalCode\",df4[\"PostalCode\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"Country\",df4[\"Country\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"HomePhone\",df4[\"HomePhone\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"Extension\",df4[\"Extension\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"Notes\",df4[\"Notes\"].cast(StringType()))\n",
    "df4 = df4.withColumn(\"ReportsTo\",df4[\"ReportsTo\"].cast(IntegerType()))\n",
    "df4 = df4.withColumn(\"PhotoPath\",df4[\"PhotoPath\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- LastName: string (nullable = true)\n",
      " |-- FirstName: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- TitleOfCourtesy: string (nullable = true)\n",
      " |-- BirthDate: date (nullable = true)\n",
      " |-- HireDate: date (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- PostalCode: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- HomePhone: string (nullable = true)\n",
      " |-- Extension: string (nullable = true)\n",
      " |-- Photo: string (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- ReportsTo: integer (nullable = true)\n",
      " |-- PhotoPath: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+--------------+\n",
      "|ShipperID|     CompanyName|         Phone|\n",
      "+---------+----------------+--------------+\n",
      "|        1|  Speedy Express|(503) 555-9831|\n",
      "|        2|  United Package|(503) 555-3199|\n",
      "|        3|Federal Shipping|(503) 555-9931|\n",
      "+---------+----------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df5 is dataframe for our shippers tabel\n",
    "\n",
    "df5= spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\shippers.csv\",inferSchema = True, header = True)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df5.withColumn(\"ShipperID\",df5[\"ShipperID\"].cast(IntegerType()))\n",
    "df5 = df5.withColumn(\"CompanyName\",df5[\"CompanyName\"].cast(StringType()))\n",
    "df5 = df5.withColumn(\"Phone\",df5[\"Phone\"].cast(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ShipperID: integer (nullable = true)\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df6 is dataframe for our Orders tabel\n",
    "\n",
    "df6 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\Orders.csv\",inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df6.withColumn(\"OrderID\",df6[\"OrderID\"].cast(IntegerType()))\n",
    "df6 = df6.withColumn(\"EmployeeID\",df6[\"EmployeeID\"].cast(IntegerType()))\n",
    "df6 = df6.withColumn(\"OrderDate\",df6[\"OrderDate\"].cast(DateType()))\n",
    "df6 = df6.withColumn(\"RequiredDate\",df6[\"RequiredDate\"].cast(DateType()))\n",
    "df6 = df6.withColumn(\"ShippedDate\",df6[\"ShippedDate\"].cast(DateType()))\n",
    "df6 = df6.withColumn(\"ShipVia\",df6[\"ShipVia\"].cast(IntegerType()))\n",
    "df6 = df6.withColumn(\"Freight\",df6[\"Freight\"].cast(DecimalType(10,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+------------+-----------+-------+-------+--------------------+------------------+--------+--------------+--------------+-----------+\n",
      "|OrderID|CustomerID|EmployeeID| OrderDate|RequiredDate|ShippedDate|ShipVia|Freight|            ShipName|       ShipAddress|ShipCity|    ShipRegion|ShipPostalCode|ShipCountry|\n",
      "+-------+----------+----------+----------+------------+-----------+-------+-------+--------------------+------------------+--------+--------------+--------------+-----------+\n",
      "|  10248|     VINET|         5|1996-07-04|  1996-08-01| 1996-07-16|      3|32.3800|Vins et alcools C...|59 rue de l'Abbaye|   Reims|          NULL|         51100|     France|\n",
      "|  10249|     TOMSP|         6|1996-07-05|  1996-08-16| 1996-07-10|      1|11.6100|  Toms Spezialitäten|     Luisenstr. 48| Münster|          NULL|         44087|    Germany|\n",
      "|  10250|     HANAR|         4|1996-07-08|  1996-08-05| 1996-07-12|      2|65.8300|       Hanari Carnes|       Rua do Paço|      67|Rio de Janeiro|            RJ|  05454-876|\n",
      "+-------+----------+----------+----------+------------+-----------+-------+-------+--------------------+------------------+--------+--------------+--------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- EmployeeID: integer (nullable = true)\n",
      " |-- OrderDate: date (nullable = true)\n",
      " |-- RequiredDate: date (nullable = true)\n",
      " |-- ShippedDate: date (nullable = true)\n",
      " |-- ShipVia: integer (nullable = true)\n",
      " |-- Freight: decimal(10,4) (nullable = true)\n",
      " |-- ShipName: string (nullable = true)\n",
      " |-- ShipAddress: string (nullable = true)\n",
      " |-- ShipCity: string (nullable = true)\n",
      " |-- ShipRegion: string (nullable = true)\n",
      " |-- ShipPostalCode: string (nullable = true)\n",
      " |-- ShipCountry: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+\n",
      "|CustomerID|         CompanyName|   ContactName|        ContactTitle|             Address|       City|Region|PostalCode|Country|       Phone|         Fax|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+\n",
      "|     ALFKI| Alfreds Futterkiste|  Maria Anders|Sales Representative|       Obere Str. 57|     Berlin|  NULL|     12209|Germany| 030-0074321| 030-0076545|\n",
      "|     ANATR|Ana Trujillo Empa...|  Ana Trujillo|               Owner|Avda. de la Const...|México D.F.|  NULL|     05021| Mexico|(5) 555-4729|(5) 555-3745|\n",
      "|     ANTON|Antonio Moreno Ta...|Antonio Moreno|               Owner|     Mataderos  2312|México D.F.|  NULL|     05023| Mexico|(5) 555-3932|        NULL|\n",
      "+----------+--------------------+--------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df7 is dataframe for our Customers tabel\n",
    "\n",
    "df7 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\Customers.csv\",inferSchema = True, header = True)\n",
    "df7.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- ContactName: string (nullable = true)\n",
      " |-- ContactTitle: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- PostalCode: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Fax: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+--------+--------+\n",
      "|OrderID|ProductID|UnitPrice|Quantity|Discount|\n",
      "+-------+---------+---------+--------+--------+\n",
      "|  10248|       11|     14.0|      12|     0.0|\n",
      "|  10248|       42|      9.8|      10|     0.0|\n",
      "|  10248|       72|     34.8|       5|     0.0|\n",
      "|  10249|       14|     18.6|       9|     0.0|\n",
      "|  10249|       51|     42.4|      40|     0.0|\n",
      "|  10250|       41|      7.7|      10|     0.0|\n",
      "|  10250|       51|     42.4|      35|    0.15|\n",
      "|  10250|       65|     16.8|      15|    0.15|\n",
      "|  10251|       22|     16.8|       6|    0.05|\n",
      "|  10251|       57|     15.6|      15|    0.05|\n",
      "|  10251|       65|     16.8|      20|     0.0|\n",
      "|  10252|       20|     64.8|      40|    0.05|\n",
      "|  10252|       33|      2.0|      25|    0.05|\n",
      "|  10252|       60|     27.2|      40|     0.0|\n",
      "|  10253|       31|     10.0|      20|     0.0|\n",
      "|  10253|       39|     14.4|      42|     0.0|\n",
      "|  10253|       49|     16.0|      40|     0.0|\n",
      "|  10254|       24|      3.6|      15|    0.15|\n",
      "|  10254|       55|     19.2|      21|    0.15|\n",
      "|  10254|       74|      8.0|      21|     0.0|\n",
      "+-------+---------+---------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df8 is dataframe for our order-details tabel\n",
    "\n",
    "df8 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\order-details.csv\",inferSchema = True, header = True)\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OrderID: integer (nullable = true)\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- UnitPrice: decimal(10,4) (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Discount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8 = df8.withColumn(\"UnitPrice\",df8[\"UnitPrice\"].cast(DecimalType(10,4)))\n",
    "df8 = df8.withColumn(\"Discount\",df8[\"Discount\"].cast(DoubleType()))\n",
    "df8.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------+----------+-------------------+---------+------------+------------+------------+------------+\n",
      "|ProductID|  ProductName|SupplierID|CategoryID|    QuantityPerUnit|UnitPrice|UnitsInStock|UnitsOnOrder|ReorderLevel|Discontinued|\n",
      "+---------+-------------+----------+----------+-------------------+---------+------------+------------+------------+------------+\n",
      "|        1|         Chai|         1|         1| 10 boxes x 20 bags|     18.0|          39|           0|          10|           0|\n",
      "|        2|        Chang|         1|         1| 24 - 12 oz bottles|     19.0|          17|          40|          25|           0|\n",
      "|        3|Aniseed Syrup|         1|         2|12 - 550 ml bottles|     10.0|          13|          70|          25|           0|\n",
      "+---------+-------------+----------+----------+-------------------+---------+------------+------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df9 is dataframe for our products tabel\n",
    "\n",
    "df9 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\products.csv\",inferSchema = True, header = True)\n",
    "df9.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df9.withColumn(\"UnitPrice\",df9[\"UnitPrice\"].cast(DecimalType(10,4)))\n",
    "df9 = df9.withColumn(\"Discontinued\",df9[\"Discontinued\"].cast(ByteType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ProductID: integer (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- SupplierID: integer (nullable = true)\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- QuantityPerUnit: string (nullable = true)\n",
      " |-- UnitPrice: decimal(10,4) (nullable = true)\n",
      " |-- UnitsInStock: integer (nullable = true)\n",
      " |-- UnitsOnOrder: integer (nullable = true)\n",
      " |-- ReorderLevel: integer (nullable = true)\n",
      " |-- Discontinued: byte (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- SupplierID: integer (nullable = true)\n",
      " |-- CompanyName: string (nullable = true)\n",
      " |-- ContactName: string (nullable = true)\n",
      " |-- ContactTitle: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- PostalCode: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Phone: string (nullable = true)\n",
      " |-- Fax: string (nullable = true)\n",
      " |-- HomePage: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df10 is dataframe for our suppliers tabel\n",
    "\n",
    "df10 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\suppliers.csv\",inferSchema = True, header = True)\n",
    "df10.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CategoryID: integer (nullable = true)\n",
      " |-- CategoryName: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df11 is dataframe for our categoriesk tabel\n",
    "\n",
    "df11 = spark.read.csv(r\"C:\\Users\\u\\Desktop\\intern\\categories.csv\",inferSchema = True, header = True)\n",
    "df11.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the tableName parameter specifies the table name to use for that DataFrame in the SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('tabel1')\n",
    "df2.registerTempTable('tabel2')\n",
    "df3.registerTempTable('tabel3')\n",
    "df4.registerTempTable('tabel4')\n",
    "df5.registerTempTable('tabel5')\n",
    "df6.registerTempTable('tabel6')\n",
    "df7.registerTempTable('tabel7')\n",
    "df8.registerTempTable('tabel8')\n",
    "df9.registerTempTable('tabel9')\n",
    "df10.registerTempTable('tabel10')\n",
    "df11.registerTempTable('tabel11')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+\n",
      "|ProductID|         ProductName|UnitPrice|\n",
      "+---------+--------------------+---------+\n",
      "|        1|                Chai|  18.0000|\n",
      "|        2|               Chang|  19.0000|\n",
      "|        3|       Aniseed Syrup|  10.0000|\n",
      "|       13|               Konbu|   6.0000|\n",
      "|       15|        Genen Shouyu|  15.5000|\n",
      "|       16|             Pavlova|  17.4500|\n",
      "|       19|Teatime Chocolate...|   9.2000|\n",
      "|       21| Sir Rodney's Scones|  10.0000|\n",
      "|       23|            Tunnbröd|   9.0000|\n",
      "|       25|NuNuCa Nuß-Nougat...|  14.0000|\n",
      "|       31|   Gorgonzola Telino|  12.5000|\n",
      "|       33|             Geitost|   2.5000|\n",
      "|       34|       Sasquatch Ale|  14.0000|\n",
      "|       35|      Steeleye Stout|  18.0000|\n",
      "|       36|         Inlagd Sill|  19.0000|\n",
      "|       39|    Chartreuse verte|  18.0000|\n",
      "|       40|    Boston Crab Meat|  18.4000|\n",
      "|       41|Jack's New Englan...|   9.6500|\n",
      "|       44|        Gula Malacca|  19.4500|\n",
      "|       45|         Rogede sild|   9.5000|\n",
      "+---------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select ProductID,ProductName,UnitPrice from tabel9 where UnitPrice <= 20 and Discontinued = 0').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subquery\n",
    "> This is the Subquery that we will use for our final query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|  CategoryName|sumofstock|\n",
      "+--------------+----------+\n",
      "|       Seafood|       701|\n",
      "|     Beverages|       539|\n",
      "|    Condiments|       507|\n",
      "|Dairy Products|       393|\n",
      "|   Confections|       386|\n",
      "|Grains/Cereals|       282|\n",
      "|  Meat/Poultry|       136|\n",
      "|       Produce|        74|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Discontinued = 0 implies the products which are not discontinued\n",
    "# We use our group by function to \n",
    "spark.sql('select t11.CategoryName,SUM(t9.UnitsInStock) AS sumofstock from tabel9 t9 INNER JOIN tabel11 t11 on t11.CategoryID = t9.CategoryID where t9.Discontinued = 0 GROUP BY t11.CategoryName order by sumofstock DESC').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We will  select CategoryName form tabel made by our subquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|  CategoryName|\n",
      "+--------------+\n",
      "|       Seafood|\n",
      "|     Beverages|\n",
      "|    Condiments|\n",
      "|Dairy Products|\n",
      "|   Confections|\n",
      "|Grains/Cereals|\n",
      "|  Meat/Poultry|\n",
      "|       Produce|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select CategoryName from(select t11.CategoryName,SUM(t9.UnitsInStock) AS sumofstock from tabel9 t9 INNER JOIN tabel11 t11 on t11.CategoryID = t9.CategoryID where t9.Discontinued = 0 GROUP BY t11.CategoryName order by sumofstock DESC) t99').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First we will select the customer id who has make order between july and september"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|CustomerID|\n",
      "+----------+\n",
      "|     VINET|\n",
      "|     TOMSP|\n",
      "|     HANAR|\n",
      "|     VICTE|\n",
      "|     SUPRD|\n",
      "|     HANAR|\n",
      "|     CHOPS|\n",
      "|     RICSU|\n",
      "|     WELLI|\n",
      "|     HILAA|\n",
      "|     ERNSH|\n",
      "|     CENTC|\n",
      "|     OTTIK|\n",
      "|     QUEDE|\n",
      "|     RATTC|\n",
      "|     ERNSH|\n",
      "|     FOLKO|\n",
      "|     BLONP|\n",
      "|     WARTH|\n",
      "|     FRANK|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select CustomerID from tabel6 WHERE MONTH(OrderDate) >= 7 and MONTH(OrderDate) <= 10').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> then we select id's of customer who is not in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|CustomerID|\n",
      "+----------+\n",
      "|     BOTTM|\n",
      "|     CACTU|\n",
      "|     CONSH|\n",
      "|     DRACD|\n",
      "|     EASTC|\n",
      "|     FISSA|\n",
      "|     GALED|\n",
      "|     LACOR|\n",
      "|     LAZYK|\n",
      "|     NORTS|\n",
      "|     OCEAN|\n",
      "|     PARIS|\n",
      "|     SEVES|\n",
      "|     SPECD|\n",
      "|     TRAIH|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select CustomerID from tabel7 where CustomerID not in(select CustomerID from tabel6 WHERE MONTH(OrderDate) >= 7 and MONTH(OrderDate) <= 10)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we use spark dataframe For selecting from our product tabel.\\\n",
    "  Filter function is like where clause in SQL\\\n",
    "  dfspark1 is our dataframe for this query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark1 = df9.select('ProductID','ProductName','UnitPrice').filter('UnitPrice >= 15 and UnitPrice <= 25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------+\n",
      "|ProductID|         ProductName|UnitPrice|\n",
      "+---------+--------------------+---------+\n",
      "|        1|                Chai|  18.0000|\n",
      "|        2|               Chang|  19.0000|\n",
      "|        4|Chef Anton's Caju...|  22.0000|\n",
      "|        5|Chef Anton's Gumb...|  21.3500|\n",
      "|        6|Grandma's Boysenb...|  25.0000|\n",
      "|       11|      Queso Cabrales|  21.0000|\n",
      "|       14|                Tofu|  23.2500|\n",
      "|       15|        Genen Shouyu|  15.5000|\n",
      "|       16|             Pavlova|  17.4500|\n",
      "|       22| Gustaf's Knäckebröd|  21.0000|\n",
      "|       35|      Steeleye Stout|  18.0000|\n",
      "|       36|         Inlagd Sill|  19.0000|\n",
      "|       39|    Chartreuse verte|  18.0000|\n",
      "|       40|    Boston Crab Meat|  18.4000|\n",
      "|       44|        Gula Malacca|  19.4500|\n",
      "|       49|            Maxilaku|  20.0000|\n",
      "|       50|    Valkoinen suklaa|  16.2500|\n",
      "|       55|        Pâté chinois|  24.0000|\n",
      "|       57|      Ravioli Angelo|  19.5000|\n",
      "|       65|Louisiana Fiery H...|  21.0500|\n",
      "+---------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here setting ascending = False will order our datafframe in descending order.\\\n",
    "by using limit function we get top ten expensive products unitprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark2 = df9.select('ProductName','UnitPrice').orderBy(\"UnitPrice\",ascending=False).limit(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         ProductName|UnitPrice|\n",
      "+--------------------+---------+\n",
      "|       Côte de Blaye| 263.5000|\n",
      "|Thüringer Rostbra...| 123.7900|\n",
      "|     Mishi Kobe Niku|  97.0000|\n",
      "|Sir Rodney's Marm...|  81.0000|\n",
      "|    Carnarvon Tigers|  62.5000|\n",
      "|Raclette Courdavault|  55.0000|\n",
      "|Manjimup Dried Ap...|  53.0000|\n",
      "|      Tarte au sucre|  49.3000|\n",
      "|         Ipoh Coffee|  46.0000|\n",
      "|   Rössle Sauerkraut|  45.6000|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we import all the function from our function module.\\\n",
    "and using filter we select the values which are greater than our average value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|avg(UnitPrice)|\n",
      "+--------------+\n",
      "|   28.86636364|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df9.agg({\"UnitPrice\": \"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark3 = df9.select('ProductName','ProductID').filter(df9['UnitPrice'] >= 28.86636364 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         ProductName|ProductID|\n",
      "+--------------------+---------+\n",
      "|Uncle Bob's Organ...|        7|\n",
      "|Northwoods Cranbe...|        8|\n",
      "|     Mishi Kobe Niku|        9|\n",
      "|               Ikura|       10|\n",
      "|Queso Manchego La...|       12|\n",
      "|        Alice Mutton|       17|\n",
      "|    Carnarvon Tigers|       18|\n",
      "|Sir Rodney's Marm...|       20|\n",
      "| Gumbär Gummibärchen|       26|\n",
      "|  Schoggi Schokolade|       27|\n",
      "|   Rössle Sauerkraut|       28|\n",
      "|Thüringer Rostbra...|       29|\n",
      "|  Mascarpone Fabioli|       32|\n",
      "|       Côte de Blaye|       38|\n",
      "|         Ipoh Coffee|       43|\n",
      "|Manjimup Dried Ap...|       51|\n",
      "|       Perth Pasties|       53|\n",
      "|Gnocchi di nonna ...|       56|\n",
      "|Raclette Courdavault|       59|\n",
      "|   Camembert Pierrot|       60|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Note: alias cnt be used on directly string.\\\n",
    " 1 shows product is discontinued\\\n",
    " 0 shows product is not discontinued\\\n",
    " we are using group by for counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspark4 = df9.groupBy(col(\"Discontinued\").alias('current(0) and discontiued product(1)')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-----+\n",
      "|current(0) and discontiued product(1)|count|\n",
      "+-------------------------------------+-----+\n",
      "|                                    1|    8|\n",
      "|                                    0|   69|\n",
      "+-------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfspark4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we will join 4 tabel by inner join\\\n",
    "df4 : employees tabel\\\n",
    "df6 : Orderss tabel\\\n",
    "df8 : Order-details tabel\\\n",
    "df9 : Products tabel\\\n",
    "to count distinct supplier id we have to import countDistinct function from our pyspark.sql.functions module\\\n",
    "at last we will filter the employees who has sell products more than 7 distict supplierid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|FirstName| LastName|\n",
      "+---------+---------+\n",
      "|     Anne|Dodsworth|\n",
      "|  Michael|   Suyama|\n",
      "|    Laura| Callahan|\n",
      "|    Janet|Leverling|\n",
      "|   Steven| Buchanan|\n",
      "|   Robert|     King|\n",
      "| Margaret|  Peacock|\n",
      "|   Andrew|   Fuller|\n",
      "|    Nancy|  Davolio|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "x = df8.join(df9,df8.ProductID == df9.ProductID).join(df6,df8.OrderID == df6.OrderID).join(df4,df6.EmployeeID == df4.EmployeeID).select(df4.EmployeeID,df4.FirstName,df4.LastName, df8.OrderID, df8.ProductID,df9.SupplierID).select(df4.EmployeeID,df4.FirstName,df4.LastName,df9.SupplierID).groupBy(df4.EmployeeID,df4.FirstName,df4.LastName).agg(countDistinct(df9.SupplierID).alias(\"UniqueSupplierID\")) .filter('UniqueSupplierID > 7').select(df4.FirstName,df4.LastName)                                                    \n",
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
